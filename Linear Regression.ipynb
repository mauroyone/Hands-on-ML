{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}=\\theta_{0}+\\sum_{i=1}^{n}{\\theta_{i}x_{i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\hat{y}$: predicted value\n",
    "- n: number of features\n",
    "- $x_i$: feature value\n",
    "- $\\theta_i$: $i^{th}$ model parameter\n",
    "\n",
    "Using vector notation:\n",
    "$$\\hat{y}=\\theta^{T}X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A set of parameters that minimize the Root Mean Square Error (RMSE) also minimize the Mean Square Error (MSE):\n",
    "$$MSE(\\textbf{X},h(\\theta))=\\frac{1}{m}\\sum_{i=1}^{m}\\left(\\theta^{T}X^{(i)}-y^{(i)}\\right)^{2}$$\n",
    "This is the cost function for a linear regression. To simplify, it can be noted as $MSE(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have a set of feature values and the actual output values, and we want to find the set of parameters that best describes the system, we can note this as:\n",
    "$$X^{T}\\theta=y$$\n",
    "We cannot assure that there is a set of $\\theta$ that generates $y$, but we can find the model parameters that best fit the problem:\n",
    "$$XX^{T}\\hat{\\theta}=X^{T}y$$\n",
    "$XX^{T}$ is always invertible, so:\n",
    "$$\\hat{\\theta}=(XX^{T})^{-1}X^{T}y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As an example:\n",
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.rand(100, 1) #column vector of 100 random numbers in [0,2)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1) #theta should be [4, 3] + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.09250532]\n",
      " [2.87492575]]\n"
     ]
    }
   ],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X]  # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)\n",
    "\n",
    "print(theta_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.09250532]\n",
      " [9.84235682]]\n"
     ]
    }
   ],
   "source": [
    "#using the parameters found:\n",
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "print(y_predict) #y = [min_value, max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting the model:\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X_new, y_predict, \"r-\", linewidth=2, label=\"Predictions\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.09250532]), array([[2.87492575]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The same stuff with sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_ #AKA theta[0], theta[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function.<br>\n",
    "It starts by filling Î¸ with random values (this is called random initialization). Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum.<br>\n",
    "Some considerations:<br>\n",
    "   - The minimum can be a local one, it's not guaranteed that it's the global\n",
    "   - For non-continous functions, the algorirm may not converge\n",
    "   - If some variable has a much bigger slope than the others, it will has more weight. Hence, the algoritm will try to fit it better before going to the others. It is highly recommended that the features have a similar scale\n",
    "   - The more parameters the model has, the longer it takes for the algoritm to converge\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we want to minimize the MSE function, to do it, we have to do the gradient:\n",
    "$$\\frac{\\partial}{\\partial\\theta_{j}}MSE(\\theta)=\\frac{2}{m}\\sum_{i=1}^{m}\\left(\\theta^{T}X^(i)-y^{(i)}\\right)x_{j}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient vector, noted $\\nabla_{\\theta}MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter):\n",
    "$$\\nabla_{\\theta}MSE(\\theta)=\\frac{2}{m}X^{T}\\left(X\\theta-y\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\color{red}{\\text{At each step, Gradient Descent algoritm uses the whole Train Set. This is why the algoritm is slow for large amount of training data.}}$\n",
    "\n",
    "However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hundreds of thousands of features is much faster using Gradient Descent than using the Normal Equation or SVD decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the gradient is available, the iteration part comes next:\n",
    "$$\\theta^{(k+1)}=\\theta^{(k)}-\\eta MSE(\\theta^{(k)})$$\n",
    "Where $\\eta$ represents the *learning rate* and the minus operator means that we are moving against the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.09250532]\n",
      " [2.87492575]]\n"
     ]
    }
   ],
   "source": [
    "eta = 0.1  # learning rate\n",
    "n_iterations = 1000 #max iterations\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients\n",
    "    \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learning rate is low, the algoritm takes too long to converge<br>\n",
    "If it's high, it may diverge<br>\n",
    "Finding a proper value is possible with a grid search, limitating the amount of iterations to discard those values that takes too much time to converge.\n",
    "\n",
    "One improvement is to add a $\\epsilon$ value that represent the variation $\\theta$ has in this iteration, if it's lower than a threshold you can break the iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "What if, instead of picking the whole train set each iteration, we rendomly pick just one?<br>\n",
    "**Obviously**, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every iteration. It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration <br>\n",
    "**On the other hand**, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down.<br>\n",
    "\n",
    "\n",
    "Randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to gradually reduce the learning rate. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.06296659]\n",
      " [2.86356196]]\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50  # learning schedule hyperparameters\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1)  # random initialization\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients\n",
    "        \n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.03292513] [2.81493828]\n"
     ]
    }
   ],
   "source": [
    "#with sklearn:\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1)\n",
    "sgd_reg.fit(X, y.ravel())\n",
    "\n",
    "print(sgd_reg.intercept_, sgd_reg.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variant is **Mini-batch Gradient Descent**. This variation rendomly takes a set of training values each iteration.\n",
    "It's a combination of the previous two algoritms. It's faster than Batch and performs better than Stochastic.<br>\n",
    "But it may not converge in the minimum and it can be stuck in a local point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
